# Notes and Resources

- TensorFlow, [A Neural Network Playground](https://playground.tensorflow.org)
- Nielsen, [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html)
- Zhang et al, [Dive into Deep Learning](https://d2l.ai/index.html)
- [Deep Learning Tutorials](http://deeplearning.net/tutorial/)
- Daumé III, [A Course in Machine Learning](http://ciml.info/)
    - Chapter 4, 10
- Berkeley DS100, [Principles and Techniques of Data Science](https://www.textbook.ds100.org)
    - Chapter 11
- Dalpiaz, [R for Statistical Learning](https://daviddalpiaz.github.io/r4sl/)
    - Chapter 28
- UFLDL
    - [Stochastic Gradient Descent](http://deeplearning.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/)
    - [Multi-Layer Neural Network](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/)
- Harvard AM207
    - [Gradient Descent](http://am207.info/wiki/gradientdescent.html)
    - [MLP as universal approximator](http://am207.info/wiki/FuncTorch.html)
    - [How sigmoids combine](http://am207.info/wiki/nnreg.html)
    - [Classification using Multi-Layer Perceptrons](http://am207.info/wiki/MLP_Classification.html)
- Stanford CS229
    - [Gradient Descent](http://cs229.stanford.edu/notes-spring2019/Gradient_Descent_Viz.pdf)
    - [Backpropagation](http://cs229.stanford.edu/notes-spring2019/backprop.pdf)
    - [Deep Learning](http://cs229.stanford.edu/notes-spring2019/cs229-notes-deep_learning.pdf)
- Stanford CS231n
    - [Stochastic Gradient Descent](https://cs231n.github.io/optimization-1/)
    - [Backpropagation](https://cs231n.github.io/)
    - Neural Networks ([Part I](https://cs231n.github.io/neural-networks-1/), [II](https://cs231n.github.io/neural-networks-2/), [III](https://cs231n.github.io/neural-networks-3/), [example](https://cs231n.github.io/neural-networks-case-study/))
    - [Convolutional Neural Networks](http://cs231n.github.io/convolutional-networks/)
- fast.ai
    - [Gradient Descent](http://wiki.fast.ai/index.php/Gradient_Descent)
    - [Linear Algebra for Deep Learning](http://wiki.fast.ai/index.php/Linear_Algebra_for_Deep_Learning)
    - [Calculus for Deep Learning](http://wiki.fast.ai/index.php/Calculus_for_Deep_Learning)
- Distill, [Why Momentum Really Works](https://distill.pub/2017/momentum/)
- Ruder, [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/)
- Parr & Howard, [The Matrix Calculus You Need For Deep Learning](https://explained.ai/matrix-calculus/index.html)
- Wiecki, [Hierarchical Bayesian Neural Networks with Informative Priors](https://twiecki.io/blog/2018/08/13/hierarchical_bayesian_neural_network/)
- Boney, [Theoretical Motivations for Deep Learning](https://rinuboney.github.io/2015/10/18/theoretical-motivations-deep-learning.html)
- Skalski
    - [Deep Dive into Math Behind Deep Networks](https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba)
    - [Preventing Deep Neural Network from Overfitting](https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a)
    - [Let’s code a Neural Network in plain NumPy](https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795)
    - [How to train Neural Network faster with optimizers?](https://towardsdatascience.com/how-to-train-neural-network-faster-with-optimizers-d297730b3713)
- Olah et al.
    - [Neural Networks, Types, and Functional Programming](http://colah.github.io/posts/2015-09-NN-Types-FP/)
    - [Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)
    - [Calculus on Computational Graphs: Backpropagation](http://colah.github.io/posts/2015-08-Backprop/)
    - [Deep Learning, NLP, and Representations](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)
    - [Conv Nets: A Modular Perspective](http://colah.github.io/posts/2014-07-Conv-Nets-Modular/)
    - [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    - [Four Experiments in Handwriting with a Neural Network](https://distill.pub/2016/handwriting/)
    - [Feature Visualization](https://distill.pub/2017/feature-visualization/)
    - [The Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/)
    - [Visualizing Representations: Deep Learning and Human Beings](http://colah.github.io/posts/2015-01-Visualizing-Representations/)
- scikit-learn, [Stochastic Gradient Descent](https://scikit-learn.org/stable/modules/sgd.html)
- Barrat, [art-DCGAN](https://github.com/robbiebarrat/art-DCGAN)

---

# Books
- Goodfellow, I., Y. Bengio, and A. Courville. (2016). *Deep Learning*, MIT Press. [[book website](https://www.deeplearningbook.org/)]
